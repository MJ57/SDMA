{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## CAUTION: this packet is mandatory for Twint working ##\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# rest of the code\n",
    "import twint\n",
    "import pandas as pd\n",
    "import preprocessor as p\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import seaborn as sns\n",
    "\n",
    "import os \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import sklearn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Scrapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_scrap(movie_name):\n",
    "    \n",
    "    c = twint.Config()\n",
    "    #c.Username = \"apple\"\n",
    "    c.Search =  \"#{}\".format(movie_name)\n",
    "    #c.Search =  \"#Mandalorien\"\n",
    "    #c.Format = \"Tweet: {tweet}\"\n",
    "    c.Hide_output = True\n",
    "    c.Lang = \"en\"\n",
    "    c.Limit = 400\n",
    "    c.Store_csv = True \n",
    "    ## Custom the output ##\n",
    "    c.Custom[\"Tweet\"] = [\"tweet\"]\n",
    "    #c.Custom[\"Date\"] = [\"date\"]\n",
    "\n",
    "    c.Output = \"{}.csv\".format(movie_name)\n",
    "    #c.Pandas = True\n",
    "    \n",
    "    twint.run.Search(c)\n",
    "    warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet cleaning - Preprocess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clean(dataframe):\n",
    "    #Remove non ASCII characters:\n",
    "    dataframe = dataframe.encode('ascii','ignore')\n",
    "    dataframe = dataframe.decode('utf8')\n",
    "    \n",
    "    #Remove URLs : \n",
    "    dataframe = re.sub('http\\S+\\s*','',dataframe)  \n",
    "    \n",
    "    \n",
    "    #removing mentions:\n",
    "    dataframe = re.sub('rt|cc','',dataframe) #Remove RT and cc\n",
    "    dataframe = re.sub('#\\S+','',dataframe) #Remove hashtags\n",
    "    \n",
    "    dataframe = dataframe.lower() #Converting to lowercase\n",
    "    #dataframe = re.sub(r'\\d+', '', dataframe) #Removing numbers\n",
    "    dataframe = re.sub(r'[^a-zA-Z0-9\\s]', '', dataframe) #Removing punctuations\n",
    "    dataframe = dataframe.strip() #remove white spaces\n",
    "    dataframe = re.sub('pictw\\S+\\s*','',dataframe)\n",
    "    \n",
    "    #Tokenize: \n",
    "    word_tokens = word_tokenize(dataframe)\n",
    "    \n",
    "    #Words stops\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "\n",
    "    filtered_tweet = []\n",
    "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "     \n",
    "    return ' '.join(filtered_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tweet_clean = df_tweet['tweet'].apply(Clean)\n",
    "#preproctest(df_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment  Prediction with TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from IPython.display import Markdown, display\n",
    "# Pretty printing the result\n",
    "def printmd(string, color=None):\n",
    "    colorstr = \"<span style='color:{}'>{}</span>\".format(color, string)\n",
    "    display(Markdown(colorstr))\n",
    "    \n",
    "def Senti_Blob(df_tweet_clean):\n",
    "    \n",
    "    senti_blob = []\n",
    "    for tweet in df_tweet_clean:\n",
    "        #print(tweet)\n",
    "        analysis = TextBlob(tweet)\n",
    "        #print(analysis.sentiment)\n",
    "        #senti.append(analysis.sentiment)\n",
    "        if analysis.sentiment[0]>0:\n",
    "            #printmd('Positive', color=\"green\")\n",
    "            senti_blob.append('positive')\n",
    "        elif analysis.sentiment[0]<0:\n",
    "            #printmd('Negative', color=\"red\")\n",
    "            senti_blob.append('negative')\n",
    "        else:\n",
    "            #printmd(\"Neutre\", color=\"grey\")\n",
    "            senti_blob.append('neutre')\n",
    "            #print(\"\")\n",
    "\n",
    "    File_sentiblob = {'Tweet': df_tweet_clean,\n",
    "            'Sentiment': senti_blob,\n",
    "           }\n",
    "\n",
    "    df_sentiblob = pd.DataFrame(File_sentiblob, columns= ['Tweet', 'Sentiment'])\n",
    "    \n",
    "    return df_sentiblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tweet['tweet'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File_sentiblob = {'Tweet': df_tweet_clean,\n",
    "#            'Sentiment': senti_blob,\n",
    "#           }\n",
    "\n",
    "#df_sentiblob = pd.DataFrame(File, columns= ['Tweet', 'Sentiment'])\n",
    "#df_sentiblob   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Emotion Analyses  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sentiblob.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.countplot(x='Sentiment',data=df_sentiblob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#count_sentiblob = df_sentiblob.groupby(\"Sentiment\", sort='count').size().reset_index(name='count')\n",
    "#print('\\n Sentiment with Sentiblob: \\n\\n', count_sentiblob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB(X_train, X_test, Y_train):\n",
    "\n",
    "    # fit the training dataset on the NB classifier\n",
    "    Naive = MultinomialNB()\n",
    "    Naive.fit(X_train, Y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_NB = Naive.predict(X_test)\n",
    "    \n",
    "    #print(predictions_NB)\n",
    "    print(f\"\\n NB CLASSIFIER: \\n emotion positive : {list(predictions_NB).count('positive')} , negative : {list(predictions_NB).count('negative')} , neutral : {list(predictions_NB).count('neutral')} \" )\n",
    "    return predictions_NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(X_train, X_test, Y_train):\n",
    "    \n",
    "    SVM = SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(X_train, Y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(X_test)\n",
    "\n",
    "    #print(predictions_SVM)\n",
    "    print(f\"\\n SVM CLASSIFIER: \\n emotion positive : {list(predictions_SVM).count('positive')} , negative : {list(predictions_SVM).count('negative')} , neutral : {list(predictions_SVM).count('neutral')}\" )\n",
    "    return predictions_SVM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFC(X_train, X_test, Y_train):\n",
    "    \n",
    "    RFC = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "    RFC.fit(X_train, Y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_RFC = RFC.predict(X_test)\n",
    "\n",
    "    print(f\"\\n RFC CLASSIFIER: \\n emotion positive : {list(predictions_RFC).count('positive')} , negative : {list(predictions_RFC).count('negative')}, neutral : {list(predictions_RFC).count('neutral')} \" )\n",
    "    return predictions_RFC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR(X_train, X_test, Y_train ):\n",
    "    \n",
    "    logreg = LogisticRegression(solver='liblinear', multi_class='auto')\n",
    "    logreg.fit(X_train, Y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_LR = logreg.predict(X_test)\n",
    "\n",
    "    print(f\"\\n Logistic Regression CLASSIFIER: \\n emotion positive : {list(predictions_LR).count('positive')} , negative : {list(predictions_LR).count('negative')} , neutral : {list(predictions_LR).count('neutral')}\" )\n",
    "    return predictions_LR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Api(movie_name):\n",
    "    tweet_scrap(movie_name)  ### A commenter pour juste prendre les csv pre enregistres\n",
    "    df_tweet = pd.read_csv(\"{}.csv\".format(movie_name))\n",
    "    df_tweet_clean = df_tweet['tweet'].apply(Clean)\n",
    "    #Senti_Blob(df_tweet_clean)\n",
    "    df_sentiment_blob = Senti_Blob(df_tweet_clean)\n",
    "    print('\\n \\n')\n",
    "    print(df_sentiment_blob)\n",
    "    \n",
    "    count_senti = df_sentiment_blob.count()\n",
    "    print('\\n \\n')\n",
    "    print('Total number of tweet: \\n',count_senti)\n",
    "    \n",
    "    sns.countplot(x='Sentiment', data=df_sentiment_blob)\n",
    "    \n",
    "    count_sentiblob = df_sentiment_blob.groupby(\"Sentiment\", sort='count').size().reset_index(name='count')\n",
    "    print('\\n #####  Sentiment with Sentiblob: ###### \\n\\n', count_sentiblob)\n",
    "    \n",
    "    ## Pourcentage \n",
    "    \n",
    "    nb_tot = df_sentiment_blob['Sentiment'].shape[0]\n",
    "    positive_sentiblob = list(df_sentiment_blob['Sentiment']).count('positive')\n",
    "    negative_sentiblob = list(df_sentiment_blob['Sentiment']).count('negative')\n",
    "    neutral_sentiblob = list(df_sentiment_blob['Sentiment']).count('neutre')\n",
    "\n",
    "    print('\\n percent of positive:', (positive_sentiblob/nb_tot)*100)\n",
    "    print( '\\n percent of negative:', (negative_sentiblob/nb_tot)*100)\n",
    "    print('\\n percent of neutre:', (neutral_sentiblob/nb_tot)*100)\n",
    "    \n",
    "    \n",
    "    #### PLOT pourcent Senti BLOB ##\n",
    "    \n",
    "    labels = ['positive', 'neutral', 'negative']\n",
    "    pred_NB = [(positive_sentiblob/nb_tot)*100, (neutral_sentiblob/nb_tot)*100, (negative_sentiblob/nb_tot)*100 ]\n",
    "    \n",
    "\n",
    "    x = np.arange(len(labels))  # the label locationsc\n",
    "    width = 0.45  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x , pred_NB, width)\n",
    "   \n",
    "    \n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Percent')\n",
    "    ax.set_title('Sentiment Percentage Prediction with SentiBlob')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    #######################################\n",
    "    \n",
    "    \n",
    "    data_clean = df_tweet_clean\n",
    "    \n",
    "    ## Training file\n",
    "    train_data = pd.read_csv('train.csv', encoding='utf-8')\n",
    "    train_data = train_data[['Category', 'Tweet']]\n",
    "    train_data.rename(columns={'Category': 'emotion'}, inplace=True)\n",
    "    train_data = train_data[train_data['emotion'] != 'Tweet']\n",
    "    train_data = train_data[train_data['Tweet'] != \"Not Available\"]\n",
    "    train_data_clean = train_data['Tweet'].apply(Clean)\n",
    "    \n",
    "    \n",
    "    sns.countplot(x='emotion',data=train_data)\n",
    "    \n",
    "    #Vectorizer : \n",
    "    count_vectorizer = CountVectorizer()\n",
    "    X_train = count_vectorizer.fit_transform(train_data_clean)\n",
    "    Y_train = train_data['emotion']\n",
    "    X_test = count_vectorizer.transform(data_clean)\n",
    "\n",
    "    #print(X_train.shape, Y_train.shape, X_test.shape)\n",
    "    \n",
    "    print('\\n #####  Sentiment Prediction with CLASSIFIER : ###### \\n\\n')\n",
    "    \n",
    "    ## Classifier ####\n",
    "    \n",
    "    predictions_NB = NB(X_train, X_test, Y_train)\n",
    "    predictions_SVM = SVM(X_train, X_test, Y_train)\n",
    "    predictions_RFC = RFC(X_train, X_test, Y_train)\n",
    "    predictions_LR = LR(X_train, X_test, Y_train)\n",
    "    \n",
    "    ### Percent calcul\n",
    "    nb_NB = predictions_NB.shape[0]\n",
    "    positive_NB = list(predictions_NB).count('positive')\n",
    "    negative_NB = list(predictions_NB).count('negative')\n",
    "    neutral_NB = list(predictions_NB).count('neutral')\n",
    "    \n",
    "    positive_NBp = (positive_NB/nb_NB)*100\n",
    "    negative_NBp = (negative_NB/nb_NB)*100\n",
    "    neutral_NBp = (neutral_NB/nb_NB)*100\n",
    "    \n",
    "    print(\"\\n NB :\")\n",
    "    print( 'Positive:', positive_NBp, '\\n Negative:', negative_NBp, '\\n Neutre:', neutral_NBp)\n",
    "\n",
    "    nb_SVM = predictions_SVM.shape[0]\n",
    "    positive_SVM = list(predictions_SVM).count('positive')\n",
    "    negative_SVM = list(predictions_SVM).count('negative')\n",
    "    neutral_SVM = list(predictions_SVM).count('neutral')\n",
    "    \n",
    "    positive_SVMp = (positive_SVM/nb_SVM)*100\n",
    "    negative_SVMp = (negative_SVM/nb_SVM)*100\n",
    "    neutral_SVMp = (neutral_SVM/nb_SVM)*100\n",
    "    \n",
    "    print(\"\\n SVM :\")\n",
    "    print( 'Positive:', positive_SVMp , '\\n Negative:', negative_SVMp , '\\n Neutre:', neutral_SVMp)\n",
    "    \n",
    "    nb_RFC = predictions_RFC.shape[0]\n",
    "    positive_RFC = list(predictions_RFC).count('positive')\n",
    "    negative_RFC = list(predictions_RFC).count('negative')\n",
    "    neutral_RFC = list(predictions_RFC).count('neutral')\n",
    "    \n",
    "    \n",
    "    positive_RFCp = (positive_RFC/nb_RFC)*100\n",
    "    negative_RFCp = (negative_RFC/nb_RFC)*100\n",
    "    neutral_RFCp = (neutral_RFC/nb_RFC)*100\n",
    "    \n",
    "    print(\"\\n RFC :\")\n",
    "    print( 'Positive:', positive_RFCp, '\\n Negative:', negative_RFCp, '\\n Neutre:', neutral_RFCp)\n",
    "\n",
    "    nb_LR = predictions_LR.shape[0]\n",
    "    positive_LR = list(predictions_LR).count('positive')\n",
    "    negative_LR = list(predictions_LR).count('negative')\n",
    "    neutral_LR = list(predictions_LR).count('neutral')\n",
    "    \n",
    "    positive_LRp = (positive_LR/nb_LR)*100\n",
    "    negative_LRp = (negative_LR/nb_LR)*100\n",
    "    neutral_LRp = (neutral_LR/nb_LR)*100\n",
    "    \n",
    "    \n",
    "    print(\"\\n LR :\")\n",
    "    print( 'Positive:', positive_LRp, '\\n Negative:', negative_LRp, '\\n Neutre:', neutral_LRp)\n",
    "    \n",
    "\n",
    "    aver_pos = (((positive_NB/nb_NB)+(positive_SVM/nb_SVM)+(positive_LR/nb_LR))/3)*100\n",
    "    aver_neg = (((negative_NB/nb_NB)+(negative_SVM/nb_SVM)+(negative_LR/nb_LR))/3)*100\n",
    "    aver_neut = (((neutral_NB/nb_NB)+(neutral_SVM/nb_SVM)+(neutral_LR/nb_LR))/3)*100\n",
    "   \n",
    "    print(\"\\n\\n Average :\")\n",
    "    print( '\\n Positive:', aver_pos, '\\n Negative:',aver_neg, '\\n Neutre:', aver_neut)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #### PLOT  CLASSIFIER COMPARAISON  ####\n",
    "    \n",
    "    labels = ['positive', 'negative', 'neutral']\n",
    "    pred_NB = [positive_NBp, negative_NBp, neutral_NBp]\n",
    "    pred_SVM = [positive_SVMp, negative_SVMp, neutral_SVMp]\n",
    "    pred_RFC = [positive_RFCp, negative_RFCp, neutral_RFCp]\n",
    "    pred_LR = [positive_LRp, negative_LRp, neutral_LRp]\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locationsc\n",
    "    width = 0.45  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x - width/2, pred_NB, width/4, label='NB')\n",
    "    rects2 = ax.bar(x - width/4, pred_SVM, width/4, label='SVM')\n",
    "    rects3 = ax.bar(x + width/2, pred_RFC, width/4, label='RFC')\n",
    "    rects4 = ax.bar(x + width/4, pred_LR, width/4, label='LR')\n",
    "    \n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Percent')\n",
    "    ax.set_title('Sentiment Percentage Prediction for each classifier')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "     #### PLOT  AVERAGE  ####\n",
    "       \n",
    "    labels = ['positive', 'negative', 'neutral']\n",
    "    pred_NB = [aver_pos, aver_neg, aver_neut ]\n",
    "    \n",
    "\n",
    "    x = np.arange(len(labels))  # the label locationsc\n",
    "    width = 0.45  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x , pred_NB, width)\n",
    "   \n",
    "    \n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Percent')\n",
    "    ax.set_title(' Average: Sentiment Percentage Prediction ')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return aver_pos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movie_name = \"Spectre\"\n",
    "#movie_name = \"avengersendgame\"\n",
    "#movie_name = \"peakyblinders\"\n",
    "#Api(movie_name)\n",
    "\n",
    "\n",
    "#movie_name = \"hobgoblins\"\n",
    "aver_mov = Api(movie_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aver_mov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imdb \n",
    "import pandas as pd \n",
    "import re  \n",
    "\n",
    "ia = imdb.IMDb()\n",
    "movie_list=[]\n",
    "\n",
    "for name in ('8111088', '0068646', '0089280', '5690360', '10326928', '6105098', '8380776', '2379713'):\n",
    "    movie = ia.get_movie(name) \n",
    "    name = movie['title']\n",
    "    score = movie.get('rating')\n",
    "    #print(\"Name :\",name,\", Score :\", score)\n",
    "    movie_list.append((name,score))\n",
    "\n",
    "df = pd.DataFrame(movie_list, columns = ['Name', 'Score']) \n",
    "\n",
    "def Clean_title(data):\n",
    "    data=data.lower()\n",
    "    data = data.replace(' ', '')\n",
    "    data= re.sub(r'[^a-zA-Z0-9\\s]', '', data)\n",
    "    return data\n",
    "\n",
    "df_name_clean = df['Name'].apply(Clean_title)\n",
    "\n",
    "\n",
    "j=0\n",
    "for i in df_name_clean:\n",
    "    print(\" --------------------- the analyses for the movie/serie:\", df['Name'][j], \"is : \")\n",
    "    aver_mov = Api(i)\n",
    "    print(\"The score for the movie/serie \",df['Name'][j] ,\"from iMDb is: \",df['Score'][j], \"\\n\\n \")\n",
    "    j=j+1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
